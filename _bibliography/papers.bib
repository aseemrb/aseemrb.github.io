---
---

@string{TCS = "Theoretical Computer Science"}
@String{JMLR = "Journal of Machine Learning Research"}
@string{ICML = "International Conference on Machine Learning"}
@string{ICLR = "International Conference on Learning Representations"}
@string{PMLR = "Proceedings of Machine Learning Research"}
@string{NeurIPS = "Neural Information Processing Systems"}
@string{LNCS = "Lecture Notes in Computer Science"}
@string{DMTCS = "Discrete Mathematics \& Theoretical Computer Science"}

@article{optimality-mp:2023,
  selected={true},
  abbr={NeurIPS},
  title={Optimality of Message-Passing Architectures for Sparse Graphs},
  author={A. Baranwal and K. Fountoulakis and A. Jagannath},
  journal={Thirty-seventh Conference on }#NeurIPS,
  preprint={https://arxiv.org/abs/2305.10391},
  url={https://openreview.net/forum?id=d1knqWjmNt},
  year={2023},
  code={https://github.com/opallab/optimality-mp-archs-sparse-graphs},
  abstract={We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is \(O(1)\)\ in the number of nodes, in the fixed-dimensional asymptotic regime, i.e., the dimension of the feature data is fixed while the number of nodes is large. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical convolution in the regime of high graph signal. Furthermore, we prove a corresponding non-asymptotic result.}
}

@article{gat-retrospective:2023,
  selected={true},
  abbr={JMLR},
  title={Graph Attention Retrospective},
  author={K. Fountoulakis and A. Levi and S. Yang and A. Baranwal and A. Jagannath},
  journal=JMLR,
  year    = {2023},
  volume  = {24},
  number  = {246},
  pages   = {1--52},
  preprint={https://jmlr.org/papers/v24/22-125.html},
  url={https://jmlr.org/papers/v24/22-125.html},
  code={https://github.com/opallab/Graph-Attention-Retrospective},
  talk={https://youtu.be/qcRSny2ecZU},
  abstract={Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular models is graph attention networks. They were introduced to allow a node to aggregate information from features of neighbor nodes in a non-uniform way, in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we theoretically study the behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here, the node features are obtained from a mixture of Gaussians and the edges from a stochastic block model. We show that in an "easy" regime, where the distance between the means of the Gaussians is large enough, graph attention is able to distinguish inter-class from intra-class edges. Thus it maintains the weights of important edges and significantly reduces the weights of unimportant edges. Consequently, we show that this implies perfect node classification. In the "hard" regime, we show that every attention mechanism fails to distinguish intra-class from inter-class edges. In addition, we show that graph attention convolution cannot (almost) perfectly classify the nodes even if intra-class edges could be separated from inter-class edges. Beyond perfect node classification, we provide a positive result on graph attention's robustness against structural noise in the graph. In particular, our robustness result implies that graph attention can be strictly better than both the simple graph convolution and the best linear classifier of node features. We evaluate our theoretical results on synthetic and real-world data.}
}

@article{effects-gc:2023,
  selected={true},
  abbr={ICLR},
  title={Effects of Graph Convolutions in Multi-layer Networks},
  author={A. Baranwal and K. Fountoulakis and A. Jagannath},
  journal=ICLR,
  preprint={https://arxiv.org/abs/2204.09297},
  url={https://openreview.net/forum?id=P-73JPgRs0R},
  slides={Effects-GC-2023.pdf},
  talk={https://youtu.be/AtsaBVu-Oq0},
  code={https://github.com/opallab/Effects-of-Graph-Convs-in-Deep-Nets},
  year={2023 (spotlight)},
  abstract={We present a rigorous theoretical understanding of the effects of graph convolutions in multi-layer networks. We study these effects through the node classification problem of a non-linearly separable Gaussian mixture model coupled with a stochastic block model. First, we show that a single graph convolution expands the regime of the distance between the means where multi-layer networks can classify the data by a factor of at least \(1/\sqrt[4]{\Delta}\), where \(\Delta\)\ denotes the expected degree of a node. Second, we show that with a slightly stronger graph density, two graph convolutions improve this factor to at least \(1/\sqrt[4]{n}\), where \(n\)\ is the number of nodes in the graph. Finally, we provide both theoretical and empirical insights into the performance of graph convolutions placed in different combinations among the layers of a network, concluding that the performance is mutually similar for all combinations of the placement. We present extensive experiments on both synthetic and real-world data that illustrate our results.}
}

@InProceedings{graph-conv:2021,
  selected={true},
  abbr={ICML},
  title={Graph convolution for Semi-Supervised Classification: Separability and OoD Generalization},
  author={Baranwal, A. and Fountoulakis, K. and Jagannath, A.},
  booktitle={Proceedings of the 38th }#ICML,
  pages={684--693},
  year={2021 (spotlight)},
  editor={Meila, Marina and Zhang, Tong},
  volume={139},
  series=PMLR,
  month={18--24 Jul},
  publisher=PMLR,
  preprint={https://arxiv.org/abs/2102.06966},
  url={http://proceedings.mlr.press/v139/baranwal21a.html},
  slides={Graph-Conv-ICML-2021.pdf},
  code={https://github.com/opallab/icml-21-graph-conv/},
  talk={https://youtu.be/2IyzIIwmQCo},
  poster={icml-2021-graph-conv.png},
  abstract={We study the classification of a mixture of Gaussians, where the data corresponds to the node attributes of a stochastic block model. We show that graph convolution extends the regime in which the data is linearly separable by a factor of roughly \(1/\sqrt D\), where \(D\)\ is the expected degree of a node, as compared to the mixture model data on its own. Furthermore, we find that the linear classifier obtained by minimizing the cross-entropy loss after the graph convolution generalizes to out-of-distribution data where the unseen data can have different intra- and inter-class edge probabilities from the training data.}
}

@article{antisq-crit-exp:2023,
  abbr={DMTCS},
  title={{Antisquares and Critical Exponents}},
  author={A. Baranwal and J. Currie and L. Mol and P. Ochem and N. Rampersad and J. Shallit},
  journal=DMTCS,
  volume={{25:2}},
  preprint={https://arxiv.org/abs/2209.09223},
  url={http://dmtcs.episciences.org/12192},
  year={2023},
  abstract={The complement \(\bar\{x\}\)\ of a binary word \(x\)\ is obtained by changing each \(0\)\ in \(x\)\ to \(1\)\ and vice versa. An antisquare is a nonempty word of the form \(x\bar\{x\}\). In this paper, we study infinite binary words that do not contain arbitrarily large antisquares. For example, we show that the repetition threshold for the language of infinite binary words containing exactly two distinct antisquares is \((5+\sqrt{5})/2\). We also study repetition thresholds for related classes, where "two" in the previous sentence is replaced by a large number. We say a binary word is good if the only antisquares it contains are \(01\)\ and \(10\). We characterize the minimal antisquares, that is, those words that are antisquares but all proper factors are good. We determine the growth rate of the number of good words of length \(n\)\ and determine the repetition threshold between polynomial and exponential growth for the number of good words.}
}

@article{ostr:2020,
  abbr={TCS},
  title={Ostrowski-Automatic Sequences: Theory and Applications},
  author={Baranwal, A. and Schaeffer, L. and Shallit, J.},
  journal=TCS,
  volume={858},
  pages={122-142},
  year={2021},
  issn={0304-3975},
  doi={https://doi.org/10.1016/j.tcs.2021.01.018},
  url={https://doi.org/10.1016/j.tcs.2021.01.018},
  code={https://github.com/aseemrb/walnut},
  year={2021},
  publisher={Elselvier},
  abstract={We extend the notion of \(k\)-automatic sequences to Ostrowski-automatic sequences, and develop a procedure to computationally decide certain combinatorial and enumeration questions about such sequences that can be expressed as predicates in first-order logic. Our primary contribution is the design and implementation of an adder recognizing addition in a generalized Ostrowski numeration system. We also provide applications of our work to several topics in combinatorics on words.},
}

@article{baranwal:2020,
  abbr={THESIS},
  title={Decision Algorithms for Ostrowski-Automatic Sequences},
  author={A. Baranwal},
  journal={MMath Thesis, Univeristy of Waterloo},
  year={2020},
  url={https://hdl.handle.net/10012/15845},
  slides={Ostrowski-Decision-Algorithms.pdf},
  abstract={We extend the notion of automatic sequences to a broader class, the Ostrowski-automatic sequences. We develop a procedure for computationally deciding certain combinatorial and enumeration questions about such sequences that can be expressed as predicates in first-order logic. In Chapter 1, we begin with topics and ideas that are preliminary to this work, including a small introduction to non-standard positional numeration systems and the relationship between words and automata. In Chapter 2, we define the theoretical foundations for recognizing addition in a generalized Ostrowski numeration system and formalize the general theory that develops our decision procedure. Next, in Chapter 3, we show how to implement these ideas in practice, and provide the implementation as an integration to the automatic theorem-proving software package -- Walnut. Further, we provide some applications of our work in Chapter 4. These applications span several topics in combinatorics on words, including repetitions, pattern-avoidance, critical exponents of special classes of words, properties of Lucas words, and so forth. Finally, we close with open problems on decidability and higher-order numeration systems and discuss future directions for research.},
}

@inproceedings{rich:2019,
  abbr={WORDS},
  title={Repetitions in infinite palindrome-rich words},
  author={Baranwal, A. and Shallit, J.},
  booktitle={Combinatorics on Words},
  series=LNCS,
  volume={11682},
  pages={93--105},
  url={https://doi.org/10.1007/978-3-030-28796-2_7},
  preprint={https://arxiv.org/abs/1904.10028},
  slides={Repetitions-Rich-Words.pdf},
  year={2019},
  organization={Springer},
  abstract={Rich words are those containing the maximum possible number of distinct palindromes. Several characteristic properties of rich words have been studied; yet the analysis of repetitions in rich words still involves some interesting open problems. We consider lower bounds on the repetition threshold of infinite rich words over 2- and 3-letter alphabets, and construct a candidate infinite rich word over the alphabet \(\Sigma_2=\\{0,1\\}\)\ with a small critical exponent of \(2+\sqrt{2}/2\). This represents the first progress on an open problem of Vesti from 2017.}
}

@inproceedings{balanced:2019,
  abbr={WORDS},
  title={Critical exponent of infinite balanced words via the Pell number system},
  author={Baranwal, A. and Shallit, J.},
  booktitle={Combinatorics on Words},
  series=LNCS,
  volume={11682},
  pages={80--92},
  url={https://doi.org/10.1007/978-3-030-28796-2_6},
  preprint={https://arxiv.org/abs/1902.00503},
  year={2019},
  organization={Springer},
  abstract={In a recent paper of Rampersad et al., the authors conjectured that the smallest possible critical exponent of an infinite balanced word over a \(5\)-letter alphabet is \(3/2\). We prove this result, using a formulation of first-order logic, the Pell number system, and a machine computation based on finite-state automata.}
}
