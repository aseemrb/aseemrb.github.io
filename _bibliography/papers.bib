---
---

@string{
  tcs = {Theoretical Computer Science}
}
@string{
  icml = {International Conference on Machine Learning}
}
@string{
  pmlr = {Proc. Mach. Learn. Res. (PMLR)}
}
@string{
  nips = {NIPS}
}
@string{
  lncs = {Lect. Notes Comput. Sci. (LNCS)}
}

@article{gat-retrospective:2022,
  abbr = {arXiv},
  title = {Graph Attention Retrospective},
  author = {Kimon Fountoulakis and Amit Levi and Shenghao Yang and Aseem Baranwal and Aukosh Jagannath},
  journal = {arXiv preprint arXiv:2202.13060},
  preprint = {https://arxiv.org/abs/2202.13060},
  pdf = 	 {https://arxiv.org/pdf/2202.13060},
  url = {https://arxiv.org/abs/2202.13060},
  code = {https://github.com/opallab/Graph-Attention-Retrospective},
  year = {2022},
  abstract = {Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular type of models is graph attention networks. These models were introduced to allow a node to aggregate information from the features of neighbor nodes in a non-uniform way in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we study theoretically this expected behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here the features of the nodes are obtained from a mixture of Gaussians and the edges from a stochastic block model where the features and the edges are coupled in a natural way. First, we show that in an "easy" regime, where the distance between the means of the Gaussians is large enough, graph attention maintains the weights of intra-class edges and significantly reduces the weights of the inter-class edges. As a corollary, we show that this implies perfect node classification independent of the weights of inter-class edges. However, a classical argument shows that in the "easy" regime, the graph is not needed at all to classify the data with high probability. In the "hard" regime, we show that every attention mechanism fails to distinguish intra-class from inter-class edges. We evaluate our theoretical results on synthetic and real-world data.}
}

@InProceedings{graph-conv:2021,
  abbr={ICML},
  title = 	 {Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization},
  author =       {Baranwal, Aseem and Fountoulakis, Kimon and Jagannath, Aukosh},
  booktitle = 	 {ICML},
  pages = 	 {684--693},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 pmlr,
  month = 	 {18--24 Jul},
  publisher = pmlr,
  preprint = {https://arxiv.org/abs/2102.06966},
  pdf = 	 {http://proceedings.mlr.press/v139/baranwal21a/baranwal21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/baranwal21a.html},
  code =   {https://github.com/opallab/icml-21-graph-conv/},
  video = {https://youtu.be/2IyzIIwmQCo},
  poster = {icml-2021-graph-conv.png},
  abstract={Recently there has been increased interest in semi-supervised classification in the presence of graphical information. A new class of learning models has emerged that relies, at its most basic level, on classifying the data after first applying a graph convolution. To understand the merits of this approach, we study the classification of a mixture of Gaussians, where the data corresponds to the node attributes of a stochastic block model. We show that graph convolution extends the regime in which the data is linearly separable by a factor of roughly \(1/\sqrt D\), where \(D\)\ is the expected degree of a node, as compared to the mixture model data on its own. Furthermore, we find that the linear classifier obtained by minimizing the cross-entropy loss after the graph convolution generalizes to out-of-distribution data where the unseen data can have different intra- and inter-class edge probabilities from the training data.}
}

@article{ostr:2020,
  abbr={TCS},
  title={Ostrowski-Automatic Sequences: Theory and Applications},
  author={Baranwal, A. and Schaeffer, L. and Shallit, J.},
  journal=tcs,
  volume = {858},
  pages = {122-142},
  year = {2021},
  issn = {0304-3975},
  doi = {https://doi.org/10.1016/j.tcs.2021.01.018},
  url={https://doi.org/10.1016/j.tcs.2021.01.018},
  code = {https://github.com/aseemrb/walnut},
  year={2021},
  publisher={Elselvier},
  abstract={We extend the notion of \(k\)-automatic sequences to Ostrowski-automatic sequences, and develop a procedure to computationally decide certain combinatorial and enumeration questions about such sequences that can be expressed as predicates in first-order logic. Our primary contribution is the design and implementation of an adder recognizing addition in a generalized Ostrowski numeration system. We also provide applications of our work to several topics in combinatorics on words.},
}

@mastersthesis{baranwal:2020,
  abbr={THESIS},
  title={Decision Algorithms for Ostrowski-Automatic Sequences},
  abstract={We extend the notion of automatic sequences to a broader class, the Ostrowski-automatic sequences. We develop a procedure for computationally deciding certain combinatorial and enumeration questions about such sequences that can be expressed as predicates in first-order logic. In Chapter 1, we begin with topics and ideas that are preliminary to this work, including a small introduction to non-standard positional numeration systems and the relationship between words and automata. In Chapter 2, we define the theoretical foundations for recognizing addition in a generalized Ostrowski numeration system and formalize the general theory that develops our decision procedure. Next, in Chapter 3, we show how to implement these ideas in practice, and provide the implementation as an integration to the automatic theorem-proving software package -- Walnut. Further, we provide some applications of our work in Chapter 4. These applications span several topics in combinatorics on words, including repetitions, pattern-avoidance, critical exponents of special classes of words, properties of Lucas words, and so forth. Finally, we close with open problems on decidability and higher-order numeration systems and discuss future directions for research.},
  url={https://hdl.handle.net/10012/15845},
  pdf={https://uwspace.uwaterloo.ca/bitstream/handle/10012/15845/Baranwal_Aseem.pdf},
  slides={Ostrowski-Decision-Algorithms.pdf},
  year={2020},
  school={University of Waterloo},
  type={thesis},
  thesistype={Master of Mathematics Thesis},
}

@inproceedings{rich:2019,
  abbr={WORDS},
  title={Repetitions in infinite palindrome-rich words},
  author={Baranwal, A. and Shallit, J.},
  abstract={Rich words are those containing the maximum possible number of distinct palindromes. Several characteristic properties of rich words have been studied; yet the analysis of repetitions in rich words still involves some interesting open problems. We consider lower bounds on the repetition threshold of infinite rich words over 2- and 3-letter alphabets, and construct a candidate infinite rich word over the alphabet \(\Sigma_2=\\{0,1\\}\)\ with a small critical exponent of \(2+\sqrt{2}/2\). This represents the first progress on an open problem of Vesti from 2017.},
  booktitle={Combinatorics on Words},
  series = lncs,
  volume = {11682},
  pages={93--105},
  url={https://doi.org/10.1007/978-3-030-28796-2_7},
  pdf={https://arxiv.org/pdf/1904.10028.pdf},
  slides={Repetitions-Rich-Words.pdf},
  year={2019},
  organization={Springer}
}

@inproceedings{balanced:2019,
  abbr={WORDS},
  title={Critical exponent of infinite balanced words via the Pell number system},
  author={Baranwal, A. and Shallit, J.},
  abstract={In a recent paper of Rampersad et al., the authors conjectured that the smallest possible critical exponent of an infinite balanced word over a \(5\)-letter alphabet is \(3/2\). We prove this result, using a formulation of first-order logic, the Pell number system, and a machine computation based on finite-state automata.},
  booktitle={Combinatorics on Words},
  series=lncs,
  volume={11682},
  pages={80--92},
  url={https://doi.org/10.1007/978-3-030-28796-2_6},
  pdf={https://arxiv.org/pdf/1902.00503.pdf},
  year={2019},
  organization={Springer}
}
