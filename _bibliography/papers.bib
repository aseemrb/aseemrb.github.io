---
---

@string{TCS = "Theor. Comput. Sci."}
@String{JMLR = "J. Mach. Learn. Res."}
@string{ICML = "International Conference on Machine Learning"}
@string{PMLR = "Proc. Mach. Learn. Res."}
@string{NeurIPS = "Neural Information Processing Systems"}
@string{LNCS = "Lect. Notes Comput. Sci."}

@article{effects-gc-deep-nets:2022,
  selected={true},
  abbr={arXiv},
  title={Effects of Graph Convolutions in Deep Networks},
  author={A. Baranwal and K. Fountoulakis and A. Jagannath},
  journal={arXiv preprint},
  preprint={https://arxiv.org/abs/2204.09297},
  url={https://arxiv.org/abs/2204.09297},
  code={https://github.com/opallab/Effects-of-Graph-Convs-in-Deep-Nets},
  year={2022},
  abstract={We present a rigorous theoretical understanding of the effects of graph convolutions in multi-layer networks. We study these effects through the node classification problem of a non-linearly separable Gaussian mixture model coupled with a stochastic block model. First, we show that a single graph convolution expands the regime of the distance between the means where multi-layer networks can classify the data by a factor of at least \(1/\sqrt[4]{\Delta}\), where \(\Delta\)\ denotes the expected degree of a node. Second, we show that with a slightly stronger graph density, two graph convolutions improve this factor to at least \(1/\sqrt[4]{n}\), where \(n\)\ is the number of nodes in the graph. Finally, we provide both theoretical and empirical insights into the performance of graph convolutions placed in different combinations among the layers of a network, concluding that the performance is mutually similar for all combinations of the placement. We present extensive experiments on both synthetic and real-world data that illustrate our results.}
}

@article{gat-retrospective:2022,
  selected={true},
  abbr={arXiv},
  title={Graph Attention Retrospective},
  author={K. Fountoulakis and A. Levi and S. Yang and A. Baranwal and A. Jagannath},
  journal={arXiv preprint},
  preprint={https://arxiv.org/abs/2202.13060},
  url={https://arxiv.org/abs/2202.13060},
  code={https://github.com/opallab/Graph-Attention-Retrospective},
  year={2022},
  abstract={We study theoretically this expected behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here the features of the nodes are obtained from a mixture of Gaussians and the edges from a stochastic block model where the features and the edges are coupled in a natural way. First, we show that in an "easy" regime, where the distance between the means of the Gaussians is large enough, graph attention maintains the weights of intra-class edges and significantly reduces the weights of the inter-class edges. As a corollary, we show that this implies perfect node classification independent of the weights of inter-class edges. However, a classical argument shows that in the "easy" regime, the graph is not needed at all to classify the data with high probability. In the "hard" regime, we show that every attention mechanism fails to distinguish intra-class from inter-class edges. We evaluate our theoretical results on synthetic and real-world data.}
}

@InProceedings{graph-conv:2021,
  selected={true},
  abbr={ICML},
  title={Graph convolution for semi-supervised classification: separability and OoD generalization},
  author={Baranwal, A. and Fountoulakis, K. and Jagannath, A.},
  booktitle={ICML},
  pages={684--693},
  year={2021},
  editor={Meila, Marina and Zhang, Tong},
  volume={139},
  series=PMLR,
  month={18--24 Jul},
  publisher=PMLR,
  preprint={https://arxiv.org/abs/2102.06966},
  url={http://proceedings.mlr.press/v139/baranwal21a.html},
  code={https://github.com/opallab/icml-21-graph-conv/},
  video={https://youtu.be/2IyzIIwmQCo},
  poster={icml-2021-graph-conv.png},
  abstract={We study the classification of a mixture of Gaussians, where the data corresponds to the node attributes of a stochastic block model. We show that graph convolution extends the regime in which the data is linearly separable by a factor of roughly \(1/\sqrt D\), where \(D\)\ is the expected degree of a node, as compared to the mixture model data on its own. Furthermore, we find that the linear classifier obtained by minimizing the cross-entropy loss after the graph convolution generalizes to out-of-distribution data where the unseen data can have different intra- and inter-class edge probabilities from the training data.}
}

@article{ostr:2020,
  abbr={TCS},
  title={Ostrowski-Automatic Sequences: Theory and Applications},
  author={Baranwal, A. and Schaeffer, L. and Shallit, J.},
  journal=TCS,
  volume={858},
  pages={122-142},
  year={2021},
  issn={0304-3975},
  doi={https://doi.org/10.1016/j.tcs.2021.01.018},
  url={https://doi.org/10.1016/j.tcs.2021.01.018},
  code={https://github.com/aseemrb/walnut},
  year={2021},
  publisher={Elselvier},
  abstract={We extend the notion of \(k\)-automatic sequences to Ostrowski-automatic sequences, and develop a procedure to computationally decide certain combinatorial and enumeration questions about such sequences that can be expressed as predicates in first-order logic. Our primary contribution is the design and implementation of an adder recognizing addition in a generalized Ostrowski numeration system. We also provide applications of our work to several topics in combinatorics on words.},
}

@mastersthesis{baranwal:2020,
  abbr={THESIS},
  title={Decision Algorithms for Ostrowski-Automatic Sequences},
  abstract={We extend the notion of automatic sequences to a broader class, the Ostrowski-automatic sequences. We develop a procedure for computationally deciding certain combinatorial and enumeration questions about such sequences that can be expressed as predicates in first-order logic. In Chapter 1, we begin with topics and ideas that are preliminary to this work, including a small introduction to non-standard positional numeration systems and the relationship between words and automata. In Chapter 2, we define the theoretical foundations for recognizing addition in a generalized Ostrowski numeration system and formalize the general theory that develops our decision procedure. Next, in Chapter 3, we show how to implement these ideas in practice, and provide the implementation as an integration to the automatic theorem-proving software package -- Walnut. Further, we provide some applications of our work in Chapter 4. These applications span several topics in combinatorics on words, including repetitions, pattern-avoidance, critical exponents of special classes of words, properties of Lucas words, and so forth. Finally, we close with open problems on decidability and higher-order numeration systems and discuss future directions for research.},
  url={https://hdl.handle.net/10012/15845},
  pdf={https://uwspace.uwaterloo.ca/bitstream/handle/10012/15845/Baranwal_Aseem.pdf},
  slides={Ostrowski-Decision-Algorithms.pdf},
  year={2020},
  school={University of Waterloo},
  type={thesis},
  thesistype={Master of Mathematics Thesis},
}

@inproceedings{rich:2019,
  abbr={WORDS},
  title={Repetitions in infinite palindrome-rich words},
  author={Baranwal, A. and Shallit, J.},
  booktitle={Combinatorics on Words},
  series=LNCS,
  volume={11682},
  pages={93--105},
  url={https://doi.org/10.1007/978-3-030-28796-2_7},
  preprint={https://arxiv.org/abs/1904.10028},
  slides={Repetitions-Rich-Words.pdf},
  year={2019},
  organization={Springer},
  abstract={Rich words are those containing the maximum possible number of distinct palindromes. Several characteristic properties of rich words have been studied; yet the analysis of repetitions in rich words still involves some interesting open problems. We consider lower bounds on the repetition threshold of infinite rich words over 2- and 3-letter alphabets, and construct a candidate infinite rich word over the alphabet \(\Sigma_2=\\{0,1\\}\)\ with a small critical exponent of \(2+\sqrt{2}/2\). This represents the first progress on an open problem of Vesti from 2017.}
}

@inproceedings{balanced:2019,
  abbr={WORDS},
  title={Critical exponent of infinite balanced words via the Pell number system},
  author={Baranwal, A. and Shallit, J.},
  booktitle={Combinatorics on Words},
  series=LNCS,
  volume={11682},
  pages={80--92},
  url={https://doi.org/10.1007/978-3-030-28796-2_6},
  preprint={https://arxiv.org/abs/1902.00503},
  year={2019},
  organization={Springer},
  abstract={In a recent paper of Rampersad et al., the authors conjectured that the smallest possible critical exponent of an infinite balanced word over a \(5\)-letter alphabet is \(3/2\). We prove this result, using a formulation of first-order logic, the Pell number system, and a machine computation based on finite-state automata.}
}
