<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Aseem  Baranwal | Implementing PEGASOS</title>
<meta name="description" content="PhD student at Cheriton School of Computer Science @UWaterloo.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<script src="https://kit.fontawesome.com/51a68d8c36.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2016/pegasos-stochastic-grad-solver/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Aseem</span>   Baranwal
      </a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5" style="text-align:justify;">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Implementing PEGASOS</h1>
    <p class="post-meta">June 17, 2016 • Aseem</p>
  </header>

  <article class="post-content">
    <p>Here’s the <a href="http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf">original paper</a> that proposes the algorithm that we’re going to implement.</p>

<p>SVMs are a very popular classification learning tool, and in the original form, the task of learning an SVM is actually a loss minimization problem with a penalty term for the <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a> of the classifier being learned. So for a given training set of \(m\) training examples \(S = \{(x_i, y_i)\}_{i=1}^m\), where \(x_i \in \mathbb{R}^n\) and \(y_i \in \{-1, +1\}\), we want to build a minimizer for the following function (note that here \(w\) and \(x\) are vectors):</p>

\[F = \min_{w}\left(\frac{\lambda}{2}||w||^2\right) + \frac{1}{m}\sum_{(x, y) \in S}l(w; (x, y))\]

<p>where \(l\) is the loss function, given by \(l(w; (x, y)) = \max{\{0, 1 - y \langle w, x \rangle\}}\) where \(\langle w, x \rangle\) denotes the inner product of the two vectors. To have an intuitive insight into why this loss function works, let’s consider two examples:</p>

\[\langle w, x \rangle =
\begin{cases}
    +ve\text{, when } w \text{ is very similar to } x\\
    -ve\text{, when } w \text{ is quite different than } x
\end{cases}\]

<p>The variable \(y\) has the classification information, where \(1\) means belonging to the class, and \(-1\) means not belonging to the class for which the classifier is being trained. Therefore, a positive value of \(\langle w, x \rangle\) along with \(y = 1\), or a negative value of \(\langle w, x \rangle\) along with \(y = -1\) is favorable because both these cases denote that the prediction works right. It also means that the loss function will be minimized in these two scenarios (work it out if needed), which is a part of our main function \(F\) that we need to build the minimizer for.</p>

<p><strong>PEGASOS</strong> stands for <em>Primal Estimated sub-GrAdient SOlver for SVM</em>, where <em>stochastic</em> means <em>having a random probability distribution or pattern that may be analysed statistically but may not be predicted precisely.</em> Let’s dive into the algorithm and see why it’s <em>stochastic</em>. The PEGASOS algorithm performs stochastic gradient descent on the primal objective function \(F\) with a <em>carefully chosen</em> step size.</p>

<h2 id="the-basic-procedure">The basic procedure</h2>
<ul>
  <li>Initially, set \(w_1\) to be the <a href="http://mathworld.wolfram.com/ZeroVector.html">zero vector</a></li>
  <li>Iterate \(T\) times while doing the following in each iteration \(t\)
    <ul>
      <li>Choose a random training example \((x_{i_t}, y_{i_t})\) by picking \(i_t\) uniformly at random from \(\{1, 2, ... m\}\)</li>
      <li>Replace the objective funtion \(F\) with an approximation based on this training example, yielding</li>
    </ul>

\[f(w, i_t) = \frac{\lambda}{2} ||w||^2 + l(w; (x_{i_t}, y_{i_t}))\]

    <ul>
      <li>Compute the subgradient of \(f(w, i_t)\) as</li>
    </ul>

\[\nabla_t = \lambda w_t - \mathbb{I}[y_{i_t} \langle w_t, x_{i_t} \rangle &lt; 1] y_{i_t} x_{i_t}\]

    <p>Here \(\mathbb{I}\) is the indicator function, which takes the value \(1\) if its argument is true, and \(0\) otherwise, so we know that the value will be \(1\) only when \(w\) yields some non-zero loss in the example \((x, y)\)</p>
    <ul>
      <li>Update \(w_{t+1} = w_t - \eta_t\nabla_t\) with the step size \(\eta_t = 1/(\lambda t)\)</li>
    </ul>
  </li>
  <li>Output \(w_{T+1}\)</li>
</ul>

<p>Let’s try to see why this approximation is okay for a suitable \(T\). For this, we’ll look at the value of the complete gradient, and the subgradient computed with the approximated function above, and see a relation between them. The complete gradient of \(F\) will be \(\dot{F}\),</p>

\[\dot{F} = \frac{\lambda}{2} \nabla ||w||^2 + \frac{1}{m} \sum_{(x, y) \in S} l(w; (x, y))\]

<p>Now what will be the expected value of the subgradient \(\nabla\) that we computed above? To find the expected value, we observe that the example taken to approximate the objective is chosen <em>uniformly at random</em>, which means that the probability of any example being selected is \(P(e) = 1/m\). Thus the expected value of the subgradient \(\nabla\) turns out to be equal to the complete gradient of \(F\), our primal objective function. And that is why intuitively this approximation is expected to work well enough. Next section deals with <strong>mini-batch iterations</strong>, to approximate the objective with more determinism.</p>

<h2 id="mini-batch-iterations">Mini-batch iterations</h2>
<p>As an extension of the basic procedure, now we would select a subset of examples, rather than selecting a single example for approximating the objective. So for a given \(k \in \{1, 2, ... m\}\), we choose a subset of size \(k\) and approximate the objective as before. Note that \(k = 1\) is the case we already saw above. So now the objective can be written as
\(f(w, A_t) = \frac{\lambda}{2} ||w||^2 + \frac{1}{k}\sum_{i \in A_t}l(w; (x_i, y_i))\)
where \(A_t\) is the subset chosen in \(t^{th}\) iteration.</p>

<h2 id="projection-step">Projection step</h2>
<p>A potential variation in the above algorithm is that we limit the set of admissible solutions to a ball of radius \(1/\sqrt{\lambda}\). To enforce this, project \(w_t\) after each iteration onto a sphere as \(w_{t+1} = \min\{1, \frac{1/\sqrt{\lambda}}{||w_{t+1}||}\}w_{t+1}\). The revised analysis as presented in the paper does not compulsorily require this projection step. It mentions this as an optional step because no major difference was found during the experiments between the projected and the unprojected variants.</p>

<p>It is proved in the paper that the number of iterations required to obtain a solution of accuracy \(\epsilon\) is \(O(1/\epsilon)\), where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs required \(\Omega(1/\epsilon^2)\) iterations because in previously devised SVM solvers, the number of iterations also scales linearly with \(1/\lambda\), where \(\lambda\) is the regularization parameter of the SVM; while with PEGASOS, this is not the case. PEGASOS works on an approximation, so the runtime of the algorithm is not dependent on the number of training examples or with some function of \(\lambda\). It just depends on \(k\), the size of the subset we are taking, and \(T\), the number of iterations that we are making. The implemented code (in C++) will be put up later when I’m not feeling lazy.</p>

  </article>

  

</div>

    </div>

  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


  <br>
</html>
